---
title: 'APSTA-2017: Process Journal'
author: "William Spagnola"
date: "Spring 2019 Semester"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```



### 4/16/19 Data Visualization Examples

Here is a link to a data visualization example **[2018 House Forecast](https://projects.fivethirtyeight.com/2018-midterm-election-forecast/house/)**
I won't tell if I think that one is good or bad.

I also think the graph below is interesting.  
```{r  out.width = "100%"}
include_graphics('img/2016_election_map.png') 
```



### 4/15/19
1) Was able to scrape key directly from site (GOAL Accomplished)
2) Created function to convert chords to roman numerical analysis
3) Need to adjust function to account for borrowed chords and mixolydian and dorian modes

**Goals**

1) Edit convert_to_roman() function
* Allow mixolydian and dorian modes
* Detect Borrowed chords (from IV, V, and minor scales)
2) Find years for each song
* Merge with original Hot100 csv
* Find other dates by building a dataset from Spotify API with song titles and dates for each artist
3) Extract Features
* Create bigrams
* Create one set with additional features (e.g. 9th, maj7th, min7th, sus, add9) and one plain set
* May consider using indictator variable for presence of 'complex' chords
4) Start Analysis
* Start with classifier for early/late Beatles songs (This could be a more realistic topic for my main analysis)

### 4/9/19

#### What I've done so far
1) Got scraper to work.  **Note this took forever to figure out!!!**
2) Got links of top 30 artists from each decade
3) Started compiling list of songs
4) Ordered Real Book as supplement.
* May contain more older songs (50s, 60s, 70s).
* Can scan and use OCR to extract chords time permitting.

#### Goal for Next Week
1) Write algorithm to guess key
* Guessing a song's key based on the chords can be tricky.
* Will most likely start with Beatles songs and take easy to predict subset.
2) Convert Selected Beatles Songs to Scale Degree
3) Write code to convert from long to wide format so that each row represents a single song instead of a song part

#### Other Goals
1) Extract Features
* bigrams
* trigrams
2) May use two or three sets of features to compare
* One with scale degree plus major or minor
* One with scale degree plus major or minor plus other features (e.g. 7th, 6th, 9th, diminished)
* One with scale degree plus major or minor plus indicator variable for presence of more complicated chords
3) Extract features from all Beatles songs
4) Create visualizations based on features
5) Created indicator for early versus later Beatles.  May use 1962â€“1966 and 1967-1970 as definining years based on [*The Red Album*](https://en.wikipedia.org/wiki/1962%E2%80%931966) and [*The Blue Album*](https://en.wikipedia.org/wiki/1967%E2%80%931970), which were compilations albums released in 1973. 
6) Train classification algorithms to identify early from later Beatles.
* Plan use 80/20 train/test split randomly 
* Use cross validations to select best model for each set of features using training data. 
* Use out-of-sample error on test data to compare each set of features. 

#### Another Intermediate Step
1) Find song years for each song.  May be able to use Spotify API.  This may seem trivial, but it can actually be pretty complicated because:
* Differences in Capitalization (easier)
* Differences in spelling (feelin' versus feeling)
* Differences in song title (Sometimes)
* Ampersands versus ands
* Same song recorded by different artists
* Same song recorded by same artist at different times
* Same song but remastered in a later year and rereleased on different album


### 3/14/19 *Happy $\pi$ Day*

#### What I've done this week
1) I learned the basic functions in the RSelenium package, and I created a webcrawler to scrape chord information from a dynamic webpage (www.hooktheory.com)
2) I learned how to scroll down to load svg images located toward the bottom of page.  Alan helped me to understand the basic concept of webcrawlers during our group discussion on Monday.  Previously, I could not understand why I was only able to scrape the first two svg images from each page.  The reason was that only the first two images load unless you scroll down. 
3) I learned how to scrape songs and links from hooktheory.com, so I can determine which songs are available for each artist.  I can now create a vector of links for the exact locations of the pages containing the chords of each song.

#### Example Dataframe
```{r echo = F}
library(knitr)
example_df <- read.csv('data/sample_data_frame.csv')
example_df %>% 
  select(-X, -link) 
```

#### Goals for next week
1) Adjust webcrawler scrolling to load 4, 5, and 6 part songs.  Currently I have been able to load up to 3 parts of a song if a song has 3 parts. 
2) Match songs scraped with website with another database in order to determine genre, year produced, and possibly key. Song keys can be determined based on the chords, but it might be easier if I can just get this info from another source such as the Spotify API. 
3) Engineer features to assess songs.  I was thinking of creating indicator variables of 1-grams, bigrams and trigrams.  This would include information on common chord changes in each of the songs.  Then I could apply some machine learning algorithm to find clusters of songs based on the chord change features.  
4) I'm not sure if I want to include slash chords yet.  Slash chords are simply chords where the bass note (lowest note) is different from the root.  For example, a C-major chord is constructed using C-E-B with C (the root) typically being the lowest note in the chord.  If I played the same chord with E or B as the lowest note, I would write the chord either as C/E or C/B respectively.  However, it still would be considered a C major chord.
5) Convert chords into scale degrees using  **[roman numerical analysis](https://en.wikipedia.org/wiki/Roman_numeral_analysis)**.  This will help compare two songs that feature similar chord progressions but are written in different keys. 

#### Challenges
1) Hooktheory.com includes around 12,000 songs in total.  This is a lot of data; however, it doesn't include all the songs in the Billboard Hot100 from the past 50 years.
2) Hooktheory.com is skewed toward modern songs.  For example, it contains over 50 songs by Taylor Swift but only around 8 songs for Elvis.  The sparsity of songs from earlier decades may make it difficult to compare trends across different decades.  
3) Another issue is that the same song can be written differently.  Furthermore, sometimes the listed artist name includes the band name as well as the singer's, and other times it only includes the lead singer's (e.g. 'Tom Petty' vs. 'Tom Petty & the Heartbreakers').  These differences can complicate the task of linking this dataset to other sources.  
4) Need advice on how to scrape data without overloading server.  What is an appropriate wait time betwen each request?

#### Idea for Analysis
1) Start by taking the Hot100 Billboard dataset (Jaejin found a csv of this) and determine the top 5 artists from each decade (1950s to 2010s) measured by number of appearances in Hot100 grouped by decade and artist.  Then build a datset with hooktheory data based around these artists. 

